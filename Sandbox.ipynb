{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sandbox.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZegnv3wkCkKMRUdJ5YrCt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZloyenotVN/All-about-ML-/blob/main/Sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqZAlpf3V_Ym"
      },
      "source": [
        "#Линейная регрессия\n",
        "\n",
        "Давайте ограничим пространство гипотез только линейными функциями от $m + 1$ аргумента, будем считать, что нулевой признак для всех объектов равен единице $x_0 = 1$:\n",
        "$\\Large \\begin{array}{rcl} \\forall h \\in \\mathcal{H}, h\\left(\\vec{x}\\right) &=& w_0 x_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_m x_m \\\\ &=& \\sum_{i=0}^m w_i x_i \\\\ &=& \\vec{x}^T \\vec{w} \\end{array}$\n",
        "\n",
        "\n",
        "Эмпирический риск (функция стоимости) принимает форму среднеквадратичной ошибки:\n",
        "$\\Large \\begin{array}{rcl}\\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) &=& \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\vec{x}_i^T \\vec{w}_i\\right)^2 \\\\ &=& \\frac{1}{2n} \\left\\| \\vec{y} - X \\vec{w} \\right\\|_2^2 \\\\ &=& \\frac{1}{2n} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right) \\end{array}$\n",
        "\n",
        "\n",
        "строки матрицы $X$ — это признаковые описания наблюдаемых объектов. Один из алгоритмов обучения $\\mathcal{M}$ такой модели — это метод наименьших квадратов. Вычислим производную функции стоимости:\n",
        "$\\Large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} &=& \\frac{\\partial}{\\partial \\vec{w}} \\frac{1}{2n} \\left( \\vec{y}^T \\vec{y} -2\\vec{y}^T X \\vec{w} + \\vec{w}^T X^T X \\vec{w}\\right) \\\\ &=& \\frac{1}{2n} \\left(-2 X^T \\vec{y} + 2X^T X \\vec{w}\\right) \\end{array}$\n",
        "\n",
        "\n",
        "приравняем к нулю и найдем решение в явном виде:\n",
        "$\\Large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} = 0 &\\Leftrightarrow& \\frac{1}{2n} \\left(-2 X^T \\vec{y} + 2X^T X \\vec{w}\\right) = 0 \\\\ &\\Leftrightarrow& -X^T \\vec{y} + X^T X \\vec{w} = 0 \\\\ &\\Leftrightarrow& X^T X \\vec{w} = X^T \\vec{y} \\\\ &\\Leftrightarrow& \\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y} \\end{array}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWGRf88kWNEv"
      },
      "source": [
        "Поздравляю, дамы и господа, мы только что с вами вывели алгоритм машинного обучения. Реализуем же этот алгоритм. Начнем с датасета, состоящего всего из одного признака. Будем брать случайную точку на синусе и добавлять к ней шум — таким образом получим целевую переменную; признаком в этом случае будет координата $x$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8FFA4GcVKnH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}